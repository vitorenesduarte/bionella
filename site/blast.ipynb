{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise de homologias por BLAST\n",
    "\n",
    "1. [local_blastp(tag_to_files, db)](#local)\n",
    "2. [docker_blastp(directory, db)](#docker)\n",
    "3. [expasy_blastp(tag_to_files)](#expasy)\n",
    "4. [blastp(tags_and_proteins, db, type)](#blast)\n",
    "5. [write_queries_to_dir(tags_and_proteins, directory)](#write_queries_to_dir)\n",
    "6. [extract_blast_info(tag_to_files, type)](#extract_blast_info)\n",
    "7. [util.www.fetch_uniprots(ids)](#fetch_uniprots)\n",
    "8. [util.www.extract_uniprot_info(entry)](#extract_uniprot_info)\n",
    "9. [main()](#main)\n",
    "\n",
    "O objectivo desta fase era, dadas as proteínas que retirámos do ficheiro genbank, procurar sequências homólogas com BLAST. Esta fase foi iterada várias vezes à procura da melhor estratégia, e no final acabámos com várias alternativas para a execução do BLAST, que podem ser encontradas no módulo [`util.blast`](https://github.com/vitorenesduarte/bionella/blob/master/util/blast.py). As alternativas são:\n",
    "\n",
    "- __Local__\n",
    "- __Docker__\n",
    "- __ExPASy__\n",
    "\n",
    "Os resultados desta fase podem ser encontrados [aqui](blast_results.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"local\"></a>\n",
    "### Local\n",
    "\n",
    "Esta função assume que o BLAST está instalado localmente, e recebe como parâmetro qual a base de dados que deve usar. O primeiro argumento desta função `tag_to_files` é um dicionário em que as chaves são __locus_tag__ e os valores são pares em que a primeira componente é o ficheiro onde se encontra a sequência de aminoácidos para ser usada na query (__in_file__), e a segunda componente é o ficheiro onde deve ser guardado o resultado do BLAST (__out_file__).\n",
    "\n",
    "```python\n",
    "def local_blastp(tag_to_files, db):\n",
    "    \"\"\"\n",
    "    Corre o blast localmente.\n",
    "    \"\"\"\n",
    "\n",
    "    for tag in tag_to_files:\n",
    "        (in_file, out_file) = tag_to_files[tag]\n",
    "    \n",
    "        blastp_cline = NcbiblastpCommandline(\n",
    "            query=in_file,\n",
    "            db=db,\n",
    "            evalue=10,\n",
    "            outfmt=5,\n",
    "            out=out_file\n",
    "        )\n",
    "        blastp_cline()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"docker\"></a>\n",
    "### Docker\n",
    "\n",
    "Para evitar a instalação de BLAST, download de bases de dados e sua adição como base de dados para ser usada pelo BLAST, criámos uma imagem __Docker__ com a base de dados __UniProtKB - Swiss-Prot__ [retirada do site da UniProt](http://www.uniprot.org/downloads). Mais informação sobre esta imagem pode ser encontrada [aqui](docker.html).\n",
    "\n",
    "```python\n",
    "def docker_blastp(directory, db):\n",
    "    \"\"\"\n",
    "    Corre o blast numa instância docker.\n",
    "    \"\"\"\n",
    "    cmd = \"docker run -e QUERY_DIR=\" + directory \\\n",
    "                  + \" -e DB=\" + db \\\n",
    "                  + \" -v $PWD/\" + directory + \":/\" + directory \\\n",
    "                  + \" -ti vitorenesduarte/swissprot_blast\"\n",
    "\n",
    "    p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    p.wait()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"expasy\"></a>\n",
    "### ExPASy\n",
    "\n",
    "A terceira opção não corre o BLAST localmente, mas utiliza o serviço disponibilizado pelo site ExPASy.\n",
    "\n",
    "```python\n",
    "def expasy_blastp(tag_to_files):\n",
    "    \"\"\"\n",
    "    Corre o blast na expasy.\n",
    "    \"\"\"\n",
    "    for tag in tag_to_files:\n",
    "        (in_file, out_file) = tag_to_files[tag]\n",
    "        query = rw.read_file(in_file)\n",
    "        blast_result = www.expasy_blast(query)\n",
    "        rw.write_file(blast_result, out_file)\n",
    "```\n",
    "\n",
    "Esta função usa uma outra função definida no módulo [`util.www`](https://github.com/vitorenesduarte/bionella/blob/master/util/www.py) que corre o BLAST contra a base de dados __UniProtKB = Swiss-Prot + TrEMBL__, usando o serviço da ExPASy.\n",
    "\n",
    "```python\n",
    "def expasy_blast(query):\n",
    "    \"\"\"\n",
    "    Corre o blast na expasy.\n",
    "    \"\"\"\n",
    "\n",
    "    url = \"http://web.expasy.org/cgi-bin/blast/blast.pl\"\n",
    "    data = {\n",
    "        \"format\": \"xml\",\n",
    "        \"prot_db1\": \"UniProtKB\",\n",
    "        \"matrix\": \"BLOSUM62\",\n",
    "        \"showsc\": 50, # best scoring sequences\n",
    "        \"showal\": 50, # best alignments\n",
    "        \"seq\": query\n",
    "    }\n",
    "\n",
    "    response = requests.post(\n",
    "        url,\n",
    "        data=data\n",
    "    )\n",
    "\n",
    "    return response.text\n",
    "```\n",
    "\n",
    "Esta última opção permitiu-nos encontrar sequências com maior homologia do que as que estávamos a encontrar ao usar apenas a __Swiss-Prot__ localmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"blast\"></a>\n",
    "### BLAST\n",
    "\n",
    "A função que engloba estas três opções recebe como argumento uma lista de pares em que a primeira componente é a __locus_tag__ da proteína, e a segunda componente é a respectiva sequência de aminoácidos. Recebe ainda qual o tipo de BLAST, __local__, __docker__ ou __expasy__, a utilizar.\n",
    "\n",
    "```python\n",
    "def blastp(tags_and_proteins, type=\"local\"):\n",
    "    \"\"\"\n",
    "    Corre o blast para a proteínas passadas como argumento.\n",
    "\n",
    "    O argumento type é opcional e pode ter dois valores:\n",
    "        - local\n",
    "        - docker\n",
    "        - expasy\n",
    "\n",
    "    É retornado um dicionário com os resultados.\n",
    "    \"\"\"\n",
    "\n",
    "    directory = \".query_dir\"\n",
    "    tag_to_files = write_queries_to_dir(tags_and_proteins, directory)\n",
    "\n",
    "    # correr o blast\n",
    "    if type == \"local\":\n",
    "        local_blastp(tag_to_files, \"swissprot\")\n",
    "    elif type == \"docker\":\n",
    "        docker_blastp(directory, \"swissprot\")\n",
    "    elif type == \"expasy\":\n",
    "        expasy_blastp(tag_to_files)\n",
    "    else:\n",
    "        raise Exception(\"Unsupported type: \" + type)\n",
    "\n",
    "    blast_results = extract_blast_info(tag_to_files, type)\n",
    "    return blast_results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta função usa duas outras funções:\n",
    "\n",
    "<a name=\"write_queries_to_dir\"></a>\n",
    "- `write_queries_to_dir` que recebe uma lista de pares, __locus_tag__ e sequência de aminoácidos __protein__, e retorna um dicionário que mapeia __locus_tag__ para pares (__in_file__, __out_file__).\n",
    "\n",
    "```python\n",
    "def write_queries_to_dir(tags_and_proteins, directory):\n",
    "    \"\"\"\n",
    "    Grava as proteínas passadas como argumento na\n",
    "    diretoria também passada como argumento.\n",
    "    \"\"\"\n",
    "\n",
    "    tag_to_files = {}\n",
    "\n",
    "    # Apagar a diretoria e criar uma nova\n",
    "    if os.path.exists(directory):\n",
    "        shutil.rmtree(directory)\n",
    "    os.makedirs(directory)\n",
    "\n",
    "    for (tag, protein) in tags_and_proteins:\n",
    "        # Gravar a proteína num ficheiro\n",
    "        in_file = directory + \"/\" + fasta_it(tag)\n",
    "        rw.write_file(protein, in_file)\n",
    "\n",
    "        # registar esta informação num dicionário\n",
    "        out_file = directory + \"/\" + xml_it(fasta_it(tag))\n",
    "        tag_to_files[tag] = (in_file, out_file)\n",
    "\n",
    "    return tag_to_files\n",
    "```\n",
    "\n",
    "<a name=\"extract_blast_info\"></a>\n",
    "- `extract_blast_info` que dado este dicionário retorna um outro dicionário que mapeia __locus_tag__ para a lista de resultados do BLAST. Cada um destes resultados é um dicionário com:\n",
    "    - __uniprot_id__\n",
    "    - __evalue__\n",
    "    - __score__\n",
    "    - __identity__\n",
    "\n",
    "```python\n",
    "def extract_blast_info(tag_to_files, type):\n",
    "    \"\"\"\n",
    "    Extrai a informação que necessitamos dos resultados do blast.\n",
    "      - uniprot_id\n",
    "      - evalue\n",
    "      - score\n",
    "      - identity\n",
    "    \"\"\"\n",
    "\n",
    "    blast_results = {}\n",
    "\n",
    "    for tag in tag_to_files:\n",
    "        (_, out_file) = tag_to_files[tag]\n",
    "\n",
    "        ### Alguns resultados do blast, ao serem lidos de disco\n",
    "        ### na função rw.read_blast\n",
    "        ### estavam a dar erro ao fazer parsing nesta linha.\n",
    "        ### Como não necessitamos deste resultado,\n",
    "        ### optamos por removê-la.\n",
    "        regex = \".*Hsp_midline.*\"\n",
    "        rw.filter_file(regex, out_file)\n",
    "\n",
    "        handle = rw.read_blast(out_file)\n",
    "\n",
    "        result = []\n",
    "\n",
    "        for a in handle.alignments:\n",
    "            # extrair o uniprot id\n",
    "            if type in [\"local\", \"docker\"]:\n",
    "                # nos blast locais, o uniprot_id está no hit_def\n",
    "                uniprot_id = a.hit_def.split(\"|\")[1]\n",
    "            elif type == \"expasy\":\n",
    "                # nos blast expasy, o uniprot_id está no hit_id\n",
    "                uniprot_id = a.hit_id.split(\"|\")[1]\n",
    "\n",
    "            # escolher sempre o primeiro hsp\n",
    "            hsp = a.hsps[0]\n",
    "            evalue = hsp.expect\n",
    "            score = hsp.score\n",
    "            identities = hsp.identities\n",
    "            align_length = hsp.align_length\n",
    "            identity = (identities * 100) / align_length\n",
    "\n",
    "            hit = {}\n",
    "            hit[\"uniprot_id\"] = uniprot_id\n",
    "            hit[\"evalue\"] = evalue\n",
    "            hit[\"score\"] = score\n",
    "            hit[\"identity\"] = identity\n",
    "            result.append(hit)\n",
    "\n",
    "        blast_results[tag] = result\n",
    "\n",
    "    return blast_results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"fetch_uniprots\"></a>\n",
    "Nesta fase ainda precisávamos de mais informação além daquela que é possível extrair dos resultados do BLAST.\n",
    "Utilizamos o serviço extração de informação em _batch_ disponibilizado pela UniProt (mais informação [aqui](http://www.uniprot.org/help/programmatic_access)), e criámos uma função que dada uma lista de __uniprot_id__ retorna um dicionário que mapeia estes id para um outro dicionário com a informação desejada. Esta função pode ser encontrada no módulo [`util.www`](https://github.com/vitorenesduarte/bionella/blob/master/util/www.py).\n",
    "\n",
    "```python\n",
    "def fetch_uniprots(ids):\n",
    "    \"\"\"\n",
    "    Faz download da informação presente no site uniprot\n",
    "    em formato xml dada uma lista de ids uniprot.\n",
    "    \"\"\"\n",
    "\n",
    "    url = \"http://www.uniprot.org/uploadlists/\"\n",
    "    max_retries = 100\n",
    "\n",
    "    # Cada pedido à uniprot vai no máximo com 1000 ids.\n",
    "    queries  = [ids[i:i+1000] for i in range(0, len(ids), 1000)]\n",
    "\n",
    "    files = []\n",
    "\n",
    "    for query in queries:\n",
    "        query_all = \" \".join(query)\n",
    "        md5 = util.md5(query_all)\n",
    "\n",
    "        data = {\n",
    "            \"format\": \"xml\",\n",
    "            \"from\": \"ID\",\n",
    "            \"to\": \"ACC\",\n",
    "            \"uploadQuery\": query_all,\n",
    "            \"jobId\": md5\n",
    "        }\n",
    "\n",
    "        done = False\n",
    "        attempt = 0\n",
    "\n",
    "        # enquanto não acabar ou não esgotar as tentativas todas\n",
    "        while not done and attempt < max_retries:\n",
    "\n",
    "            # HTTP POST\n",
    "            response = requests.post(\n",
    "                url,\n",
    "                data=data\n",
    "            )\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                # Gravar o xml\n",
    "                file_path = \"/tmp/\" + md5 + \".xml\"\n",
    "                rw.write_file(response.text, file_path)\n",
    "                files.append(file_path)\n",
    "                done = True\n",
    "\n",
    "            attempt += 1\n",
    "\n",
    "        if not done:\n",
    "            print(\"Não consegui fazer download de \" + str(query))\n",
    "\n",
    "    infos = {}\n",
    "\n",
    "    for file_path in files:\n",
    "        tree = parse_xml(file_path, add_root=True, start=2, end=1)\n",
    "        entries = tree.findall(\".//entry\")\n",
    "\n",
    "        for entry in entries:\n",
    "            (uniprot_id, info) = extract_uniprot_info(entry)\n",
    "            infos[uniprot_id] = info\n",
    "\n",
    "    return infos\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"extract_uniprot_info\"></a>\n",
    "Podemos ver que esta função utiliza uma outra função `extract_uniprot_info`, também definida no módulo [`util.www`](https://github.com/vitorenesduarte/bionella/blob/master/util/www.py).\n",
    "\n",
    "```python\n",
    "# FINISH THIS ONCE WE DECIDE ALL THE INFO WE WANT FROM UNIPROT\n",
    "def extract_uniprot_info(entry):\n",
    "    \"\"\"\n",
    "    Extrai a informação que necessitamos da uniprot.\n",
    "      - status (reviewed, unreviewed)\n",
    "      - accessions\n",
    "      - comentários sobre a função\n",
    "      - GO - Molecular Function\n",
    "      - sequência\n",
    "    \"\"\"\n",
    "    # dataset\n",
    "    ds = entry.get(\"dataset\")\n",
    "\n",
    "    if ds == \"Swiss-Prot\":\n",
    "        status = \"reviewed\"\n",
    "    elif ds == \"TrEMBL\":\n",
    "        status = \"unreviewed\"\n",
    "    else:\n",
    "        print(\"Não conheço o dataset \" + ds)\n",
    "\n",
    "    # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"main\"></a>\n",
    "Todos estes passos podem ser encontrados no ficheiro [second.py](https://github.com/vitorenesduarte/bionella/tree/master/second.py) e resumem-se a:\n",
    "\n",
    "```python\n",
    "import util.rw as rw\n",
    "import util.blast as blast\n",
    "import util.www as www\n",
    "import util.util as util\n",
    "\n",
    "def add_info_to_blast_results(blast_results, uniprots):\n",
    "    \"\"\"\n",
    "    Esta função adiciona aos blast results informação\n",
    "    retirada do site da uniprot.\n",
    "    \"\"\"\n",
    "\n",
    "    for tag in blast_results:\n",
    "        for i in range(len(blast_results[tag])):\n",
    "            uniprot_id = blast_results[tag][i][\"uniprot_id\"]\n",
    "\n",
    "            # esta proteína aparece nos resultados do blast como\n",
    "            # Q8TC84-2 mas a uniprot retorna a sua informação\n",
    "            # como Q8TC84\n",
    "            if uniprot_id == \"Q8TC84-2\":\n",
    "                uniprot_id = \"Q8TC84\"\n",
    "\n",
    "            properties = [\n",
    "                \"molecular_functions\",\n",
    "                \"sequence\",\n",
    "                \"status\"\n",
    "            ]\n",
    "\n",
    "            for p in properties:\n",
    "                value = uniprots[uniprot_id][p]\n",
    "                blast_results[tag][i][p] = value\n",
    "\n",
    "    return blast_results\n",
    "\n",
    "def main():\n",
    "    ncbi_json_path = \".ncbi.json\"\n",
    "    blast_results_json_path = \".blast_results.json\"\n",
    "\n",
    "    dictionary = rw.read_json(ncbi_json_path)\n",
    "    tags_and_proteins = util.get_tags_and_proteins(dictionary)\n",
    "\n",
    "    blast_results = blast.blastp(\n",
    "        tags_and_proteins,\n",
    "        \"expasy\"\n",
    "    )\n",
    "\n",
    "    uniprot_ids = set()\n",
    "    for tag in blast_results:\n",
    "        for result in blast_results[tag]:\n",
    "            uniprot_ids.add(result[\"uniprot_id\"])\n",
    "\n",
    "    uniprot_ids = list(uniprot_ids)\n",
    "    uniprots = www.fetch_uniprots(uniprot_ids)\n",
    "\n",
    "    blast_results = add_info_to_blast_results(\n",
    "        blast_results,\n",
    "        uniprots\n",
    "    )\n",
    "    rw.write_json(blast_results, blast_results_json_path)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
